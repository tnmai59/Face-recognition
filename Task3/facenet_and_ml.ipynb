{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common\n",
    "from utils import *\n",
    "# Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a random\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the image dimensions\n",
    "IMG_W, IMG_H, IMG_C = (160, 160, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('D:/Face recognition/model/facenet-keras/facenet_keras.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of individuals: 1020/n\n"
     ]
    }
   ],
   "source": [
    "root_path = 'D:/Face recognition/VN-celeb/'\n",
    "\n",
    "# Collect all the person names\n",
    "dir_names = os.listdir(root_path)\n",
    "person_names = [name for name in dir_names]\n",
    "n_individuals = len(person_names)\n",
    "\n",
    "print(f\"Total number of individuals: {n_individuals}/n\")\n",
    "# print(f\"Name of the individuals : /n/t{person_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train test split: 100%|██████████| 1020/1020 [00:00<00:00, 5292.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18902, 4203)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train test split\n",
    "trainpaths, testpaths = get_train_test_split(root_path)\n",
    "len(trainpaths), len(testpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data labels:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4203/4203 [00:00<00:00, 333673.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train data labels:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18902/18902 [00:00<00:00, 254949.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating labels for train and test data\n",
    "print('Creating test data labels:')\n",
    "test_labels = generating_labels(testpaths, 'VN-celeb/(\\d+)/')\n",
    "print('Creating train data labels:')\n",
    "train_labels = generating_labels(trainpaths, 'VN-celeb/(\\d+)/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18902, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18902/18902 [4:05:06<00:00,  1.29it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for train data\n",
    "embeddings = np.empty(shape=(len(trainpaths), 128))\n",
    "print(embeddings.shape)\n",
    "# Loop over images\n",
    "for i in tqdm(range(len(trainpaths))):\n",
    "    with open(os.devnull, 'w') as f, redirect_stdout(f):\n",
    "        # Load the image\n",
    "        image = load_image(trainpaths[i])\n",
    "\n",
    "        # Generate the embedding\n",
    "        embedding = image_to_embedding(image, model)\n",
    "\n",
    "    # Store the embedding\n",
    "    embeddings[i] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4203/4203 [1:37:06<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for test data\n",
    "embeddings_test = np.empty(shape=(len(testpaths), 128))\n",
    "\n",
    "# Loop over images\n",
    "for i in tqdm(range(len(testpaths))):\n",
    "    with open(os.devnull, 'w') as f, redirect_stdout(f):\n",
    "        # Load the image\n",
    "        image = load_image(testpaths[i])\n",
    "\n",
    "        # Generate the embedding\n",
    "        embedding = image_to_embedding(image, model)\n",
    "\n",
    "    # Store the embedding\n",
    "    embeddings_test[i] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Face recognition/Task3/X_train.pkl', 'wb') as f:\n",
    "  pickle.dump(embeddings, f)\n",
    "with open('D:/Face recognition/Task3/X_test.pkl', 'wb') as f:\n",
    "  pickle.dump(embeddings_test, f)\n",
    "with open('D:/Face recognition/Task3/y_train.pkl', 'wb') as f:\n",
    "  pickle.dump(train_labels, f)\n",
    "with open('D:/Face recognition/Task3/y_test.pkl', 'wb') as f:\n",
    "  pickle.dump(test_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Task3/Data/X_train.pkl\", \"rb\") as f:   # Unpickling\n",
    "  X_train = pickle.load(f)\n",
    "with open(\"../Task3/Data/X_test.pkl\", \"rb\") as f:   # Unpickling\n",
    "  X_test = pickle.load(f)\n",
    "with open(\"../Task3/Data/y_train.pkl\", \"rb\") as f:   # Unpickling\n",
    "  y_train = pickle.load(f)\n",
    "with open(\"../Task3/Data/y_test.pkl\", \"rb\") as f:   # Unpickling\n",
    "  y_test = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23105, 128), (23105,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append((\"LR\",LogisticRegression()))\n",
    "models.append((\"NB\",GaussianNB()))\n",
    "models.append((\"KNN\",KNeighborsClassifier()))\n",
    "models.append((\"DT\",DecisionTreeClassifier()))\n",
    "models.append((\"SVM\",SVC()))\n",
    "models.append((\"RF\",RandomForestClassifier()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA mean cross validations score:0.84077\n",
      "LR mean cross validations score:0.66345\n",
      "NB mean cross validations score:0.77252\n",
      "KNN mean cross validations score:0.78329\n",
      "DT mean cross validations score:0.26345\n",
      "SVM mean cross validations score:0.81173\n",
      "RF mean cross validations score:0.72352\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "# pca=PCA(n_components=50, whiten=True)\n",
    "# pca.fit(X)\n",
    "# X_pca=pca.transform(X)\n",
    "for name, model in models:\n",
    "    kfold=KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    \n",
    "    cv_scores=cross_val_score(model, X, y, cv=kfold)\n",
    "    print(\"{} mean cross validations score:{:.5f}\".format(name, cv_scores.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
